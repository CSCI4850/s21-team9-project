{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excessive-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 1: RUN to pull all of the needed Libraries \n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "import face_recognition\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "IMAGESIZE = [200, 200]  # width (0) Height (1) Images are resized to the this before getting push to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nervous-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460594   62328\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 2: RUN to pull in the META data files \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# load up the meta data Image paths \n",
    "imdb_jsonFile = json.load(open(\"imdb_outputdata.json\"))\n",
    "wiki_jsonFile = json.load(open(\"wiki_outputdata.json\"))\n",
    "imdb_file_location = '../imdb/'\n",
    "wiki_file_location = '../wiki/'\n",
    "imdbLen = len(imdb_jsonFile)\n",
    "wikiLen = len(wiki_jsonFile)\n",
    "print(imdbLen, \" \", wikiLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alien-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 3: RUN to define all the functions needed to play with the models \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# usage print(get_face_locations('nm0000001_rm946909184_1899-5-10_1968.jpg'))\n",
    "def get_face_locations(imagePath): \n",
    "    image = face_recognition.load_image_file(imagePath)\n",
    "    return face_recognition.face_locations(image)\n",
    "\n",
    "# crop the image to just read the face location  \n",
    "def crop_image(image, face_loc):\n",
    "    return image[face_loc[1]:face_loc[3], face_loc[0]:face_loc[2]]  #  1 , 3, 0, 2\n",
    "\n",
    "# resize the image to match the \n",
    "def resize_image(image):\n",
    "    dsize = (IMAGESIZE[0], IMAGESIZE[1]) # width (0) Height (1) \n",
    "    return cv2.resize(image, dsize)\n",
    "\n",
    "# function to  fetch the IMAGES from X(start) to Y(end) and returns three arrays  \n",
    "def read_images_gender_Age(start, end, JSON_File, images_location):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    Z=[]\n",
    "    corrupted_entry = 0 \n",
    "    for x in range(start, end):\n",
    "        try:\n",
    "            image = cv2.imread(images_location+JSON_File[x][3])\n",
    "                \n",
    "                # if the image is one of the corrupted image, skip this entry\n",
    "            if(image.shape == (47,100,3) or image.shape == (1,1,3) or JSON_File[x][0] == \"nan\"):\n",
    "                corrupted_entry +=1 \n",
    "                continue\n",
    "            \n",
    "            face_loc = JSON_File[x][2]\n",
    "            cropped_image = crop_image(image, face_loc)\n",
    "            resized_image = resize_image(cropped_image)\n",
    "            X.append(resized_image)\n",
    "            Y.append(JSON_File[x][0]) # gender \n",
    "            Z.append(JSON_File[x][1]) # age\n",
    "            #print(resized_image.shape)\n",
    "            #plt.imshow(resized_image) \n",
    "            #plt.show()\n",
    "            if(x % 1000 == 0):\n",
    "                print('read one 1000')\n",
    "        except Exception as e:\n",
    "            print('ran into exception, skipping this entry, Error:', e)\n",
    "            continue\n",
    "            \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).astype('float32')\n",
    "    Y = keras.utils.to_categorical(Y)\n",
    "    Z = np.array(Z).astype('float32')\n",
    "    Z = keras.utils.to_categorical(Z, num_classes = 120)\n",
    "    print('Read from ', start, ' to ', end, '. There were ',corrupted_entry, ' corrupted entries avoided')\n",
    "    return X,Y,Z\n",
    "\n",
    "#store the model and the history and which data entries were processed \n",
    "def store_history_model(model, history, start, end, duration, historyFilelocation, model_name):\n",
    "    try:\n",
    "        try:\n",
    "            json_object = json.load(open(historyFilelocation))\n",
    "        except:\n",
    "            json_object = []\n",
    "\n",
    "        storage_file = open(historyFilelocation, 'w')\n",
    "\n",
    "        h5filename = str(start) + '-' + str(end)+ model_name + '.h5'\n",
    "        h5_location = \"./\" + model_name + \"/\" + h5filename;\n",
    "\n",
    "        model.save(h5_location);\n",
    "\n",
    "        json_object.append({\"h5Filename\": h5filename,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"duration\": duration,\n",
    "                \"h5_location\": h5_location, \n",
    "                \"categorical_accuracy\": str(history.history['categorical_accuracy']),          \n",
    "                \"val_categorical_accuracy\": str(history.history['val_categorical_accuracy']),          \n",
    "                \"loss\": str(history.history['loss']),\n",
    "                \"val_loss\": str(history.history['val_loss'])\n",
    "                           })\n",
    "\n",
    "        json.dump(json_object, storage_file)\n",
    "        storage_file.close()\n",
    "    except Exception as e:\n",
    "        print('ran into exception while trying to store model after fitting for index ', start, ' to index ', end, ' , Error:', e)\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# pull the latest model checkpoint    \n",
    "def pull_latest_model(historyFilelocation):\n",
    "    try:\n",
    "        json_object = json.load(open(historyFilelocation))\n",
    "    except Exception as e:\n",
    "        print('could not get the latest model, Error:', e)\n",
    "        return 0\n",
    "    return keras.models.load_model(json_object[len(json_object)-1]['h5_location'])\n",
    "\n",
    "#get the base gender model                                  \n",
    "def gender_model(X,Y):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(10, 10),\n",
    "                                  activation='relu',\n",
    "                                  input_shape=[X.shape[1],\n",
    "                                               X.shape[2],\n",
    "                                               X.shape[3]]))\n",
    "    model.add(keras.layers.Conv2D(128, (10, 10), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#get the base age model                                   \n",
    "def age_model(X,Y):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(10, 10),\n",
    "                                  activation='relu',\n",
    "                                  input_shape=[X.shape[1],\n",
    "                                               X.shape[2],\n",
    "                                               X.shape[3]]))\n",
    "    model.add(keras.layers.Conv2D(128, (10, 10), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    model.summary()\n",
    "    return model                                \n",
    "                                   \n",
    "def fit_model(model, X, Y):\n",
    "    batch_size = 25\n",
    "    epochs = 1\n",
    "    validation_split = 0.2\n",
    "    history = model.fit(X, Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split= validation_split)\n",
    "    return history\n",
    "\n",
    "#pick up from where you last left off and start fitting\n",
    "#start_time = time.time()\n",
    "#time.sleep(75)\n",
    "#print((time.time() - start_time))\n",
    "def train_models(start, end, JSON_FILE, images_location, historyFile ):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        X,Y,Z = read_images_gender_Age(start, end, JSON_FILE, images_location) #\"../imdb/\"\n",
    "        latest_gender_model =  pull_latest_model(historyFile)  # \"./genderHistory.json\"\n",
    "        if(latest_gender_model == 0):\n",
    "            print(\"couldn't find a stored model, generating a new one\")\n",
    "            latest_gender_model = gender_model(X,Y)\n",
    "        else:\n",
    "            print(\"Found previously stored model\")\n",
    "        \n",
    "        history = fit_model(latest_gender_model, X, Y)\n",
    "        duration = (time.time() - start_time)\n",
    "        store_history_model(latest_gender_model,history,start,end,duration,historyFile,'gender')\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print('ran into exception while training index ', start, ' to index ', end, ' did not train this set, Error:', e)\n",
    "        return 0\n",
    "        \n",
    "    return 1 \n",
    "\n",
    "#fetch the last 20 percent from the imdb(368,475  --  92,119) & wiki(49,862 --- 12,466) data sets, and run model.predict and see what's the accuracy rate.     \n",
    "def test_model():\n",
    "    \n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 4: Train and play with the models\n",
    "\n",
    "\"\"\" \n",
    "    \n",
    "X,Y,Z = read_images_gender_Age(0,50, imdb_jsonFile, \"../imdb/\") \n",
    "latest_gender_model =  pull_latest_model(\"./genderHistory.json\")\n",
    "if(latest_gender_model == 0):\n",
    "    print(\"couldn't find a stored model, generating a new one\")\n",
    "    latest_gender_model = gender_model(X,Y)\n",
    "else:\n",
    "    print(\"Found previously stored model\")\n",
    "history = fit_model(latest_gender_model)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "honest-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "read one 1000\n",
      "Read from  40000  to  60000 . There were  3  corrupted entries avoided\n",
      "Found previously stored model\n",
      "640/640 [==============================] - 7718s 12s/step - loss: 0.6632 - categorical_accuracy: 0.6337 - val_loss: 0.5900 - val_categorical_accuracy: 0.7433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my_latest = pull_latest_model(\"./genderHistory.json\")\n",
    "train_models(60000, 80000, imdb_jsonFile, \"../imdb/\", \"./genderHistory.json\")\n",
    "#print(my_latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "micro-detroit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 199, 199, 64)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 198, 198, 128)     32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1254528)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               160579712 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 160,630,210\n",
      "Trainable params: 160,630,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(2, 2),\n",
    " activation='relu',\n",
    " input_shape=[X.shape[1],\n",
    " X.shape[2],\n",
    " X.shape[3]]))\n",
    "model.add(keras.layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    " optimizer=keras.optimizers.Adam(),\n",
    " metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-wages",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "accurate-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 92s 3s/step - loss: 818.7046 - categorical_accuracy: 0.6572 - val_loss: 0.7737 - val_categorical_accuracy: 0.6927\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 76s 2s/step - loss: 0.5627 - categorical_accuracy: 0.7848 - val_loss: 0.9803 - val_categorical_accuracy: 0.6976\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 74s 2s/step - loss: 0.4561 - categorical_accuracy: 0.8305 - val_loss: 0.8663 - val_categorical_accuracy: 0.6976\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.3172 - categorical_accuracy: 0.8636 - val_loss: 0.9948 - val_categorical_accuracy: 0.7073\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 78s 2s/step - loss: 0.2567 - categorical_accuracy: 0.9027 - val_loss: 1.8021 - val_categorical_accuracy: 0.7171\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.1457 - categorical_accuracy: 0.9543 - val_loss: 1.8486 - val_categorical_accuracy: 0.7268\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.0941 - categorical_accuracy: 0.9781 - val_loss: 3.1627 - val_categorical_accuracy: 0.7415\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.0690 - categorical_accuracy: 0.9796 - val_loss: 6.4938 - val_categorical_accuracy: 0.7317\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.1210 - categorical_accuracy: 0.9801 - val_loss: 2.5325 - val_categorical_accuracy: 0.7317\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.2213 - categorical_accuracy: 0.9653 - val_loss: 2.0087 - val_categorical_accuracy: 0.7317\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split= validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "revised-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions  = model.predict(X[:,:,:,:])\n",
    "men_classified_as_women = []\n",
    "women_classified_as_men = []\n",
    "for x in range(len(Y)):\n",
    "    if(float(Y[x][1]) == 1.0 and float(predicitions[x][1]) < 0.5):\n",
    "        men_classified_as_women.append(x)\n",
    "        \n",
    "for x in range(len(Y)):\n",
    "    if(float(Y[x][0]) == 1.0 and float(predicitions[x][0]) < 0.5):\n",
    "        women_classified_as_men.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('men_classified_as_women ', len(men_classified_as_women))\n",
    "print('women_classified_as_men ',  len(women_classified_as_men))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(women_classified_as_men)):\n",
    "    print('showing Image at index -', women_classified_as_men[x])\n",
    "    print(Y[women_classified_as_men[x]])\n",
    "    print(predicitions[women_classified_as_men[x]])\n",
    "    plt.imshow(X[women_classified_as_men[x],:,:,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(imdb_jsonFile) THIS IS JUST FOR TESTTTTING \n",
    "counter = 0 \n",
    "corrupted_images = 0 \n",
    "\n",
    "for x in imdb_jsonFile:\n",
    "    if counter < 4:\n",
    "        print(x)\n",
    "        print(x[3])\n",
    "        image = cv2.imread(x[3])\n",
    "        \n",
    "        # if the image is one of the corrupted image, skip this entry\n",
    "        if(image.shape == (47,100,3) or image.shape == (1,1,3)):\n",
    "            corrupted_images +=1 \n",
    "            continue\n",
    "        \n",
    "        face_loc = x[2]\n",
    "        cropped_image = crop_image(image, face_loc)\n",
    "        resized_image = resize_image(cropped_image)\n",
    "        print(resized_image.shape)\n",
    "        plt.imshow(resized_image) \n",
    "        plt.show()\n",
    "    counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organic-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#time.sleep(75)\n",
    "#print((time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reverse-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   1000  saving\n",
      "1001   2000\n",
      "2001   3000\n",
      "3001   4000\n",
      "4001   5000\n",
      "5001   6000\n",
      "6001   7000\n",
      "7001   8000\n",
      "8001   9000\n",
      "9001   10000\n",
      "10001   11000\n",
      "11001   12000\n",
      "12001   13000\n",
      "13001   14000\n",
      "14001   15000\n",
      "15001   16000\n",
      "16001   17000\n",
      "17001   18000\n",
      "18001   19000\n",
      "19001   20000\n",
      "20001   21000\n",
      "21001   22000\n",
      "22001   23000\n",
      "23001   24000\n",
      "24001   25000\n",
      "25001   26000  saving\n",
      "26001   27000\n",
      "27001   28000\n",
      "28001   29000\n",
      "29001   30000\n",
      "30001   31000\n",
      "31001   32000\n",
      "32001   33000\n",
      "33001   34000\n",
      "34001   35000\n",
      "35001   36000\n",
      "36001   37000\n",
      "37001   38000\n",
      "38001   39000\n",
      "39001   40000\n",
      "40001   41000\n",
      "41001   42000\n",
      "42001   43000\n",
      "43001   44000\n",
      "44001   45000\n",
      "45001   46000\n",
      "46001   47000\n",
      "47001   48000\n",
      "48001   49000\n",
      "49001   50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in range(0, 50000, 1000):\n",
    "    if(x % 25000 == 0):\n",
    "        print(x+1 , ' ', x+1000, ' saving')\n",
    "    else:\n",
    "        print(x+1 , ' ', x+1000)\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-spank",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
