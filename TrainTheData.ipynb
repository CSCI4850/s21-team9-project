{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 1: RUN to pull all of the needed Libraries \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "import face_recognition\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "IMAGESIZE = [200, 200]  # width (0) Height (1) Images are resized to the this before getting push to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surgical-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460594   62328\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 2: RUN to pull in the META data files \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# load up the meta data Image paths \n",
    "imdb_jsonFile = json.load(open(\"imdb_outputdata.json\"))\n",
    "wiki_jsonFile = json.load(open(\"wiki_outputdata.json\"))\n",
    "imdb_file_location = '../imdb/'\n",
    "wiki_file_location = '../wiki/'\n",
    "imdbLen = len(imdb_jsonFile)\n",
    "wikiLen = len(wiki_jsonFile)\n",
    "print(imdbLen, \" \", wikiLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "assisted-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 3: RUN to define all the functions needed to play with the models \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# usage print(get_face_locations('nm0000001_rm946909184_1899-5-10_1968.jpg'))\n",
    "def get_face_locations(imagePath): \n",
    "    image = face_recognition.load_image_file(imagePath)\n",
    "    return face_recognition.face_locations(image)\n",
    "\n",
    "# crop the image to just read the face location  \n",
    "def crop_image(image, face_loc):\n",
    "    return image[face_loc[1]:face_loc[3], face_loc[0]:face_loc[2]]  #  1 , 3, 0, 2\n",
    "\n",
    "# resize the image to match the \n",
    "def resize_image(image):\n",
    "    dsize = (IMAGESIZE[0], IMAGESIZE[1]) # width (0) Height (1) \n",
    "    return cv2.resize(image, dsize)\n",
    "\n",
    "# function to  fetch the IMAGES from X(start) to Y(end) and returns three arrays  \n",
    "def read_images_gender_Age(start, end, JSON_File, images_location):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    Z=[]\n",
    "    corrupted_entry = 0 \n",
    "    for x in range(start, end+1):\n",
    "        try:\n",
    "            image = cv2.imread(images_location+JSON_File[x][3])\n",
    "                \n",
    "                # if the image is one of the corrupted image, skip this entry\n",
    "            if(image.shape == (47,100,3) or image.shape == (1,1,3) or JSON_File[x][0] == \"nan\"):\n",
    "                corrupted_entry +=1 \n",
    "                continue\n",
    "            \n",
    "            face_loc = JSON_File[x][2]\n",
    "            cropped_image = crop_image(image, face_loc)\n",
    "            resized_image = resize_image(cropped_image)\n",
    "            X.append(resized_image)\n",
    "            Y.append(JSON_File[x][0]) # gender \n",
    "            Z.append(JSON_File[x][1]) # age\n",
    "            #print(resized_image.shape)\n",
    "            #plt.imshow(resized_image) \n",
    "            #plt.show()\n",
    "        except Exception as e:\n",
    "            print('ran into exception, skipping this entry, Error:', e)\n",
    "            continue\n",
    "            \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).astype('float32')\n",
    "    Y = keras.utils.to_categorical(Y)\n",
    "    Z = np.array(Z).astype('float32')\n",
    "    Z = keras.utils.to_categorical(Z, num_classes = 120)\n",
    "    print('Read from ', start, ' to ', end, '. There were ',corrupted_entry, ' corrupted entries avoided')\n",
    "    return X,Y,Z\n",
    "\n",
    "#store the model and the history and which data entries were processed \n",
    "def store_history_model(model, history, start, end, duration, historyFilelocation, model_name):\n",
    "    try:\n",
    "        json_object = json.load(open(historyFilelocation))\n",
    "    except:\n",
    "        json_object = []\n",
    "        \n",
    "    storage_file = open(historyFilelocation, 'w')\n",
    "    \n",
    "    h5filename = str(start) + '-' + str(end)+ model_name + '.h5'\n",
    "    h5_location = \"./\" + model_name + \"/\" + h5filename;\n",
    "    \n",
    "    model.save(h5_location);\n",
    "    \n",
    "    json_object.append({\"h5Filename\": h5filename,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"duration\": duration,\n",
    "            \"h5_location\": h5_location })\n",
    "\n",
    "    json.dump(json_object, storage_file)\n",
    "    storage_file.close()\n",
    "\n",
    "# pull the latest model checkpoint    \n",
    "def pull_latest_model(historyFilelocation):\n",
    "    try:\n",
    "        json_object = json.load(open(historyFilelocation))\n",
    "    except:\n",
    "        return 0\n",
    "    return keras.models.load_model(json_object[len(json_object)-1]['h5_location'])\n",
    "\n",
    "#get the base gender model                                  \n",
    "def gender_model(X,Y):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(2, 2),\n",
    "                                  activation='relu',\n",
    "                                  input_shape=[X.shape[1],\n",
    "                                               X.shape[2],\n",
    "                                               X.shape[3]]))\n",
    "    model.add(keras.layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#get the base age model                                   \n",
    "def age_model(X,Y):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(2, 2),\n",
    "                                  activation='relu',\n",
    "                                  input_shape=[X.shape[1],\n",
    "                                               X.shape[2],\n",
    "                                               X.shape[3]]))\n",
    "    model.add(keras.layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "    model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "    model.summary()\n",
    "    return model                                \n",
    "                                   \n",
    "def fit_model(model):\n",
    "    batch_size = 25\n",
    "    epochs = 5\n",
    "    validation_split = 0.2\n",
    "    history = model.fit(X, Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split= validation_split)\n",
    "    return history\n",
    "\n",
    "#pick up from where you last left off and start fitting\n",
    "def train_models(start, end, JSON_FILE, images_location, historyFile):\n",
    "    try:\n",
    "        X,Y,Z = read_images_gender_Age(start, end, JSON_FILE, images_location) #\"../imdb/\"\n",
    "        latest_gender_model =  pull_latest_model(historyFile)  # \"./genderHistory.json\"\n",
    "        if(latest_gender_model == 0):\n",
    "            print(\"couldn't find a stored model, generating a new one\")\n",
    "            latest_gender_model = gender_model(X,Y)\n",
    "        else:\n",
    "            print(\"Found previously stored model\")\n",
    "\n",
    "        history = fit_model(latest_gender_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print('ran into exception while training index ', start, ' to index ', end, ' moving on to the next set, Error:', e)\n",
    "        \n",
    "    return 1 \n",
    "\n",
    "#fetch the last 20 percent from the imdb(92119) & wiki(12466) data sets, and run model.predict and see what's the accuracy rate.     \n",
    "def test_model():\n",
    "    \n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cutting-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('Original.h5')\n",
    "#newModel =  keras.models.load_model('Original.h5')\n",
    "#newModel.summary()\n",
    "#print(model.layers[0].get_weights())\n",
    "#print(newModel.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "geological-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from  0  to  50 . There were  0  corrupted entries avoided\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 199, 199, 64)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 198, 198, 128)     32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1254528)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               160579712 \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 160,630,210\n",
      "Trainable params: 160,630,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "couldn't find a stored model, generating a new one\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 5s 2s/step - loss: 1209.4207 - categorical_accuracy: 0.5133 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 4s 2s/step - loss: 2309.0265 - categorical_accuracy: 0.8300 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 4s 2s/step - loss: 2648.3158 - categorical_accuracy: 0.6733 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 4s 2s/step - loss: 911.9835 - categorical_accuracy: 0.8633 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 4s 2s/step - loss: 206.6417 - categorical_accuracy: 0.8767 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "(51, 200, 200, 3)\n",
      "(51, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "STEP 4: Train and play with the models\n",
    "\n",
    "\"\"\" \n",
    "    \n",
    "X,Y,Z = read_images_gender_Age(0,50, imdb_jsonFile, \"../imdb/\") \n",
    "latest_gender_model =  pull_latest_model(\"./genderHistory.json\")\n",
    "if(latest_gender_model == 0):\n",
    "    print(\"couldn't find a stored model, generating a new one\")\n",
    "    latest_gender_model = gender_model(X,Y)\n",
    "else:\n",
    "    print(\"Found previously stored model\")\n",
    "history = fit_model(latest_gender_model)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran into exception, skipping this entry, Error: 'NoneType' object has no attribute 'shape'\n",
      "Read from  0  to  1200 . There were  178  corrupted entry avoided\n",
      "(1022, 200, 200, 3)\n",
      "(1022, 2)\n"
     ]
    }
   ],
   "source": [
    "X,Y,Z = read_images_gender_Age(0,1200,wiki_jsonFile,wiki_file_location)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "supported-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(852, 113)\n"
     ]
    }
   ],
   "source": [
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "checked-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 199, 199, 64)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 198, 198, 128)     32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 99, 99, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1254528)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               160579712 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 160,630,210\n",
      "Trainable params: 160,630,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(2, 2),\n",
    " activation='relu',\n",
    " input_shape=[X.shape[1],\n",
    " X.shape[2],\n",
    " X.shape[3]]))\n",
    "model.add(keras.layers.Conv2D(128, (2, 2), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
    " optimizer=keras.optimizers.Adam(),\n",
    " metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-airport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "quiet-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "33/33 [==============================] - 92s 3s/step - loss: 818.7046 - categorical_accuracy: 0.6572 - val_loss: 0.7737 - val_categorical_accuracy: 0.6927\n",
      "Epoch 2/10\n",
      "33/33 [==============================] - 76s 2s/step - loss: 0.5627 - categorical_accuracy: 0.7848 - val_loss: 0.9803 - val_categorical_accuracy: 0.6976\n",
      "Epoch 3/10\n",
      "33/33 [==============================] - 74s 2s/step - loss: 0.4561 - categorical_accuracy: 0.8305 - val_loss: 0.8663 - val_categorical_accuracy: 0.6976\n",
      "Epoch 4/10\n",
      "33/33 [==============================] - 83s 3s/step - loss: 0.3172 - categorical_accuracy: 0.8636 - val_loss: 0.9948 - val_categorical_accuracy: 0.7073\n",
      "Epoch 5/10\n",
      "33/33 [==============================] - 78s 2s/step - loss: 0.2567 - categorical_accuracy: 0.9027 - val_loss: 1.8021 - val_categorical_accuracy: 0.7171\n",
      "Epoch 6/10\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.1457 - categorical_accuracy: 0.9543 - val_loss: 1.8486 - val_categorical_accuracy: 0.7268\n",
      "Epoch 7/10\n",
      "33/33 [==============================] - 82s 2s/step - loss: 0.0941 - categorical_accuracy: 0.9781 - val_loss: 3.1627 - val_categorical_accuracy: 0.7415\n",
      "Epoch 8/10\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.0690 - categorical_accuracy: 0.9796 - val_loss: 6.4938 - val_categorical_accuracy: 0.7317\n",
      "Epoch 9/10\n",
      "33/33 [==============================] - 79s 2s/step - loss: 0.1210 - categorical_accuracy: 0.9801 - val_loss: 2.5325 - val_categorical_accuracy: 0.7317\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 80s 2s/step - loss: 0.2213 - categorical_accuracy: 0.9653 - val_loss: 2.0087 - val_categorical_accuracy: 0.7317\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split= validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dynamic-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitions  = model.predict(X[:,:,:,:])\n",
    "men_classified_as_women = []\n",
    "women_classified_as_men = []\n",
    "for x in range(len(Y)):\n",
    "    if(float(Y[x][1]) == 1.0 and float(predicitions[x][1]) < 0.5):\n",
    "        men_classified_as_women.append(x)\n",
    "        \n",
    "for x in range(len(Y)):\n",
    "    if(float(Y[x][0]) == 1.0 and float(predicitions[x][0]) < 0.5):\n",
    "        women_classified_as_men.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('men_classified_as_women ', len(men_classified_as_women))\n",
    "print('women_classified_as_men ',  len(women_classified_as_men))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(women_classified_as_men)):\n",
    "    print('showing Image at index -', women_classified_as_men[x])\n",
    "    print(Y[women_classified_as_men[x]])\n",
    "    print(predicitions[women_classified_as_men[x]])\n",
    "    plt.imshow(X[women_classified_as_men[x],:,:,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(imdb_jsonFile) THIS IS JUST FOR TESTTTTING \n",
    "counter = 0 \n",
    "corrupted_images = 0 \n",
    "\n",
    "for x in imdb_jsonFile:\n",
    "    if counter < 4:\n",
    "        print(x)\n",
    "        print(x[3])\n",
    "        image = cv2.imread(x[3])\n",
    "        \n",
    "        # if the image is one of the corrupted image, skip this entry\n",
    "        if(image.shape == (47,100,3) or image.shape == (1,1,3)):\n",
    "            corrupted_images +=1 \n",
    "            continue\n",
    "        \n",
    "        face_loc = x[2]\n",
    "        cropped_image = crop_image(image, face_loc)\n",
    "        resized_image = resize_image(cropped_image)\n",
    "        print(resized_image.shape)\n",
    "        plt.imshow(resized_image) \n",
    "        plt.show()\n",
    "    counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "professional-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#time.sleep(75)\n",
    "#print((time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
